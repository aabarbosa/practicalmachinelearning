---
title: "Project for Practical Machine Learning"
author: "Jacques Sauve"
date: "December 15, 2015"
output: html_document
---

# Executive Summary

This report investigates prediction models for Human Activity Recognition - HAR. 
This is crucial for the development of context-aware systems.
According to the problem statement, "There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.
Data was provided by the Pontifícia Universidade Católica from  Rio de Janeiro (PUC-Rio);
it consists of measurements from the wearable accelerometers, including values for roll, pitch, yaw, acceleration, gyroscope data, kurtosis, skewness taken from sensors located on weightlifters' arms, forearms and on the dumbbells.
The goal is to predict the manner in which they did the exercise
(Sitting, Sitting down, Standing, Standing up, Walking)

Two models were produced and compared, a random forest model on the original predictors and and another random forest on the principal components obtained from PCA.
The best model has expected testing (out-of-sample) accuracy of 99,3%.
Finally, 20 test cases were predicted and submitted for automatic evaluation.

# Exploratory Analysis
The training data are available here:https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test (used for validation) data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

We first look at the data by hand using a shell (more command) or text editor and we discover that NA can be represented by NA or "".
With this information, we can read the data in an R data frame.

```{r read, echo=FALSE, cache=TRUE, message=FALSE}
library(caret)
df = read.csv('pml-training.csv', sep=',', quote= '"', na.strings=c('NA'))
df.validation = read.csv('pml-testing.csv', sep=',', quote= '"', na.strings=c('NA'))
dim(df)
dim(df.validation)
```

- It is clear that we have enough data (19622 rows) to split the training data in a training and a testing set.
This is not strictly necessary, since the whole training data could be used with cross-validation to estimate the testing (out-of-sample) accuracy.
However, it is a recommended practice when much data is available, since it gives a more precise estimate of the testing accuracy.

Therefore, we will train on 70% of the data from pml-training.csv and 30% will be used to estimate the testing accuracy.
Final validation will be done on the 20 cases in pml-testing.csv.

## Examine data format, data preparation

It is crucial to make sure that the data is read in the correct format (factor, character, int, numeric) to allow model training to be done efficiently.

```{r eval=FALSE}
str(df)
```

Looking at str(df) reveals that:

- There are a lot of missing data: since they are concentrated in seeral columns, it is better to remove the columns, not the rows containing NA.
- Some numerical columns are converted to factors because they contain a string "#DIV/0!"
These should all be converted to numeric, the reason being that several models 
(ex. random forests) have complexity exponential in the number of factor levels and 
these numeric factors were converted to factors with hundreds of levels!

    + Here is an example:
        + kurtosis_roll_belt: Factor w/ 397 levels ...
    + This would make many model techniques essentially unusable.

```{r echo=FALSE}
# Find columns that have a certain pattern and convert them to numeric
# Since this is a new way of dealing with dataframes (for me), here is an explanation
#   With apply go over each column
#   Search if expr is in this column with grepl
#   Since we get a vector back, use any() to get TRUE if any element matches expr
#   Finally check which elements (columns) are TRUE (i.e. contain the searched expr).
```

```{r warning=FALSE}
# Find columns that have a certain pattern and convert them to numeric
convert.factor.to.numeric = function(df, expr) {
    special.numeric.columns = which(apply(df, 2, function(x) any(grepl(expr, x))))
    # convert these columns to numeric: may cause warnings due to NAs
    df[special.numeric.columns] <- 
        lapply(df[special.numeric.columns], function(x) as.numeric(as.character(x)))
    return (df)
}
df.clean = convert.factor.to.numeric(df, '#DIV/0!')
df.validation.clean = convert.factor.to.numeric(df.validation, '#DIV/0!')

# now remove columns with NAs
which.cols.need.to.go = sapply(df.clean, function(x) sum(is.na(x)) > 0)
df.clean = df.clean[!which.cols.need.to.go]
ncol(df.clean)
which.cols.need.to.go = sapply(df.validation.clean, function(x) sum(is.na(x)) > 0)
df.validation.clean = df.validation.clean[!which.cols.need.to.go]
#ncol(df.validation.clean)
```

- The first 7 predictors (IDs, dates) have nothing to do with the wearable accelerometers, so remove them

```{r}
df.clean = df.clean[-(1:7)]
#ncol(df.clean)
```

- We now have 53 predictors

# Classification Models

Reference: [caret package documentation](http://topepo.github.io/caret)

We will use the caret package, providing more than 150 models with unified a interface.

We need to choose a model technique.
We have many predictors (p=53) but we have enough data (n) not to worry about using special methods for n < p.
We first chose a random forest (RF) model because RF is considered to be one of the very best classification techniques.
The model will be trained and tested on the split data (70% training, 30% test).
Then at least one more model will be trained and compared to the RF.

## Useful functions

```{r}
calc.accuracy = function(model, testing) {
    pred = predict(model, testing)
    cm = confusionMatrix(pred, testing$classe)
    return (cm$overall['Accuracy'])
}
```

## Prepare the training and testing data

- 70% training, 30% testing

```{r cache=TRUE}
set.seed(1)
train.proportion = 0.7
trainIndex = createDataPartition(df.clean$classe, p = train.proportion, list=FALSE)
training = df.clean[trainIndex,]
testing = df.clean[-trainIndex,]
dim(training)
dim(testing)
```

## Train and test a random forest

We have made the following choices:
- Use the 'rf' method from the caret package to train a random forest;
- Accept the default of 500 trees to be grown; the randomForest (used by caret) documentation states: 
"This should not be set to too small a number, to ensure that every input row gets predicted at least a few times."
- The tuning parameter mtry 
(Number of predictors randomly sampled as candidates at each split in the tree) 
will automatically be optimized by the training algorithm during cross-validation.
- In order to keep the algorithm as fast as possible (there are many predictors), we choose:
    + a faster cross-validation method: out-of-bag (oob);
    + a value of 10 for nodesize, so that tree nodes will not contain less than 10 samples,
    making the tree smaller.
    + predictors that have near-zero-variance will be removed by pre-processing
- These decisions can be reviewed if the model accuracy is low.

```{r echo=FALSE}
# For speed:
#   cache results
#   Don't use formula: call randomForest(predictors,decision)
#   Use do.trace argument to see the OOB error in real-time; 
#       this way you may detect that you can lower ntree.
#   Check if your computer haven't run out of RAM and it is using swap space. 
#   If so, buy a bigger computer.
#   See number of levels in each factor. try to reduce if too large
```

```{r rf, cache=TRUE}
predictors = training[ , -which(names(training) %in% c("classe"))]
response = training[,'classe']
#start = Sys.time()
fit.rf <- train(predictors, response, 
                nodesize=10,
                allowParallel=TRUE, do.trace=FALSE,
                trControl = trainControl(method ='oob'),
                method="rf", preProcess=c('nzv'))
#end = Sys.time()
#print(end - start)
print(fit.rf)
ggplot(fit.rf$result, aes(x = mtry, y = Accuracy)) +
    geom_point() +
    geom_line() +
    ggtitle('Accuracy versus mtry')
varImpPlot(fit.rf$finalModel)
tr.acc = max(fit.rf$results$Accuracy)
cat('Training accuracy: ', tr.acc, '\n')
tst.acc = calc.accuracy(fit.rf, testing)
cat('Testing accuracy: ', tst.acc, '\n')
```

- The above information shows:
    + The best value of mtry found was 27 (see ouput of fit.rf and the first plot)
    + The most important predictors of the output class (roll_belt, pitch_forearm, ...)
    + The training accuracy is `r tr.acc` and the testing accuracy is `r tst.acc`

## Model with Principal Component Analysis before Random Forest

- With such accuracy, we really don't need a second model, but let us use PCA and then apply the result to a random forest to see if improvements result.

```{r pca, cache=TRUE, warning=FALSE}
#start = Sys.time()
fit.pca.rf <- train(predictors, response, 
                nodesize=10,
                allowParallel=TRUE, do.trace=FALSE,
                trControl = trainControl(method ='oob'),
                method="rf", preProcess=c('nzv', 'pca'))
#end = Sys.time()
#print(end - start)
print(fit.pca.rf)
ggplot(fit.pca.rf$result, aes(x = mtry, y = Accuracy)) +
   geom_point() +geom_line()
tr.acc.pca = max(fit.pca.rf$results$Accuracy)
cat('Training accuracy: ', tr.acc.pca, '\n')
tst.acc.pca = calc.accuracy(fit.pca.rf, testing)
cat('Testing accuracy: ', tst.acc.pca, '\n')
```

- The accuracy (`r tst.acc.pca`) is unfortunately worse and we keep the initial random forest model.

# Validation

- We now calculate the 'classe' response for the 20 validation samples.

```{r}
pred.validation = predict(fit.rf, newdata=df.validation.clean)

pml_write_files = function(x, dir){
  n = length(x)
  for(i in 1:n){
    filename = paste0(dir,'/problem_id_',i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(pred.validation, 'predictions')
```

- The final predictions for the 20 validation samples are: `r pred.validation`.
